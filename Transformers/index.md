# Transformers

Shield: [![CC BY 4.0][cc-by-shield]][cc-by]

These images were originally published in the book ["Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"](https://leanpub.com/pytorch).

They are also available at the book's official repository: [https://github.com/dvgodoy/PyTorchStepByStep](https://github.com/dvgodoy/PyTorchStepByStep).

## Index

- [Back](https://github.com/dvgodoy/dl-visuals)
- [Stacked Encoders and Decoders](#stacked-encoders-and-decoders)
- [Sub-Layer](#sub-layer)
- [Transformer](#transformer)
    - [Encoder](#encoder)
    - [Decoder](#decoder)
    - [Full](#full)
- [Special Classifier Token](#special-classifier-token)
- [Vision Transformer (ViT)](#vision-transformer)

### **** CLICK ON THE IMAGES FOR FULL SIZE ****

## Papers

- Self-Attention / Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani, A. et al. (2017)
- Vision Transformer: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy, A. et al. (2020)

## Stacked Encoders and Decoders

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/stacked_encdec.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/stacked_encdec.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/stacked_layers.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/stacked_layers.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

## Sub-Layer

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/sublayer.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/sublayer.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.3.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.3.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.5.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.5.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.6.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/eq10.6.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

## Transformer

### Encoder

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_encself.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_encself.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/enc_both.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/enc_both.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

### Decoder

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_decself.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_decself.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/dec_both.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/dec_both.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

### Full

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_encdecself.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_encdecself.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/full_transformer.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/full_transformer.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_classes.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/transf_classes.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

## Special Classifier Token

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/cls_hidden_state.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/cls_hidden_state.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/two_cls_embeds.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/two_cls_embeds.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

## Vision Transformer

[![](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/vit_model.png)](https://raw.githubusercontent.com/dvgodoy/dl-visuals/main/Transformers/vit_model.png)
*Source: [Chapter 10](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb)*

This work is licensed under a
[Creative Commons Attribution 4.0 International License][cc-by].

[![CC BY 4.0][cc-by-image]][cc-by]

[cc-by]: http://creativecommons.org/licenses/by/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png
[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg
